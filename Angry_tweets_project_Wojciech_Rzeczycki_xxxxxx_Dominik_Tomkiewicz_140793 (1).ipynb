{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b3de073b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5e451f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tk = WhitespaceTokenizer()\n",
    "\n",
    "RE_SPACES = re.compile(\"\\s+\")\n",
    "RE_HASHTAG = re.compile(\"[@#][_a-z0-9]+\")\n",
    "RE_EMOTICONS = re.compile(\"(:-?\\))|(:p)|(:d+)|(:-?\\()|(:/)|(;-?\\))|(<3)|(=\\))|(\\)-?:)|(:'\\()|(8\\))\")\n",
    "RE_HTTP = re.compile(\"http(s)?://[/\\.a-z0-9]+\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "96c9a3f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                        Not Available\n",
       "1    IOS 9 App Transport Security. Mm need to check...\n",
       "2    Mar if you have an iOS device, you should down...\n",
       "3    @jimmie_vanagon my phone does not run on lates...\n",
       "4    Not sure how to start your publication on iOS?...\n",
       "5    Two Dollar Tuesday is here with Forklift 2, Qu...\n",
       "Name: Tweet, dtype: object"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets = pd.read_csv('train.csv', header=0, sep=',').iloc[: , -1]\n",
    "tweets.head(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c52f2de3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    negative\n",
       "1     neutral\n",
       "2     neutral\n",
       "3    negative\n",
       "4    positive\n",
       "5     neutral\n",
       "Name: Category, dtype: object"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiments = pd.read_csv('train.csv', header=0, sep=',').iloc[: , -2]\n",
    "sentiments.head(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "c651a2f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer():\n",
    "    @staticmethod\n",
    "    def tokenize(tweets, sentiments):\n",
    "        zero_entry = {\"sentiment\": [''], \"tweet\": ['']}\n",
    "        normalized_tweets = pd.DataFrame(zero_entry)\n",
    "        for (i, j) in zip(tweets.index, sentiments.index):\n",
    "            tweet = BeforeTokenizationNormalizer.normalize(tweets.iat[i])\n",
    "            tweet = tk.tokenize(tweet)\n",
    "            new_row = pd.DataFrame({\"sentiment\": [sentiments.iat[j]], \"tweet\": [tweet]})\n",
    "            normalized_tweets = pd.concat([normalized_tweets, new_row], ignore_index = True)\n",
    "\n",
    "        normalized_tweets = normalized_tweets.iloc[1:]\n",
    "        return normalized_tweets\n",
    "    \n",
    "class BeforeTokenizationNormalizer():\n",
    "    @staticmethod\n",
    "    def normalize(text):\n",
    "        text = text.strip().lower()\n",
    "        text = text.replace('&nbsp;', ' ')\n",
    "        text = text.replace('&lt;', '<')\n",
    "        text = text.replace('&gt;', '>')\n",
    "        text = text.replace('&amp;', '&')\n",
    "        text = text.replace('&pound;', u'£')\n",
    "        text = text.replace('&euro;', u'€')\n",
    "        text = text.replace('&copy;', u'©')\n",
    "        text = text.replace('&reg;', u'®')\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "77222010",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     sentiment                                              tweet\n",
      "1     negative                                   [not, available]\n",
      "2      neutral  [ios, 9, app, transport, security., mm, need, ...\n",
      "3      neutral  [mar, if, you, have, an, ios, device,, you, sh...\n",
      "4     negative  [@jimmie_vanagon, my, phone, does, not, run, o...\n",
      "5     positive  [not, sure, how, to, start, your, publication,...\n",
      "...        ...                                                ...\n",
      "5966   neutral  [@youaremyarsenal, wouldn't, surprise, me, if,...\n",
      "5967   neutral  [rib, injury, for, zlatan, against, russia, is...\n",
      "5968   neutral  [noooooo!, i, was, hoping, to, see, zlatan, be...\n",
      "5969   neutral                                   [not, available]\n",
      "5970   neutral                                   [not, available]\n",
      "\n",
      "[5970 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "print(Tokenizer.tokenize(tweets, sentiments))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "923fbb3e",
   "metadata": {},
   "source": [
    "# Klasyfikator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5d81d2f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "efb38bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_set= pd.read_csv(\"train.csv\", sep=',', header = 1)\n",
    "\n",
    "classes = ['negative', 'positive', 'neutral']\n",
    "stop_words = ['ourselves', 'hers', 'between', 'yourself', 'but', 'again', 'there', 'about', 'once', 'during', 'out', 'very', 'having', 'with', 'they', 'own', 'an', 'be', 'some', 'for', 'do', 'its', 'yours', 'such', 'into', 'of', 'most', 'itself', 'other', 'off', 'is', 's', 'am', 'or', 'who', 'as', 'from', 'him', 'each', 'the', 'themselves', 'until', 'below', 'are', 'we', 'these', 'your', 'his', 'through', 'don', 'nor', 'me', 'were', 'her', 'more', 'himself', 'this', 'down', 'should', 'our', 'their', 'while', 'above', 'both', 'up', 'to', 'ours', 'had', 'she', 'all', 'no', 'when', 'at', 'any', 'before', 'them', 'same', 'and', 'been', 'have', 'in', 'will', 'on', 'does', 'yourselves', 'then', 'that', 'because', 'what', 'over', 'why', 'so', 'can', 'did', 'not', 'now', 'under', 'he', 'you', 'herself', 'has', 'just', 'where', 'too', 'only', 'myself', 'which', 'those', 'i', 'after', 'few', 'whom', 't', 'being', 'if', 'theirs', 'my', 'against', 'a', 'by', 'doing', 'it', 'how', 'further', 'was', 'here', 'than']\n",
    "\n",
    "temp = np.random.rand(len(tweets_set)) < 0.80\n",
    "tweetsTrain = tweets_set[temp]\n",
    "tweetsTest = tweets_set[~temp]\n",
    "\n",
    "tweetsTrainData = []\n",
    "tweetsTrainLabel = []\n",
    "\n",
    "tweetsTestData = []\n",
    "tweetsTestLabel = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "48ce0cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, tweet in tweetsTrain.iterrows():\n",
    "    tweet[2] = re.sub(r'@\\w+', '', tweet[2])\n",
    "    querywords = tweet[2].split()\n",
    "    resultwords  = [word for word in querywords if word.lower() not in stop_words]\n",
    "    tweet[2] = ' '.join(resultwords)\n",
    "    tweetsTrainData.append(tweet[2])\n",
    "    tweetsTrainLabel.append(tweet[1])\n",
    "\n",
    "for index, tweet in tweetsTest.iterrows():\n",
    "    tweet[2] = re.sub(r'@\\w+', '', tweet[2])\n",
    "    querywords = tweet[2].split()\n",
    "    resultwords  = [word for word in querywords if word.lower() not in stop_words]\n",
    "    tweet[2] = ' '.join(resultwords)\n",
    "    tweetsTestData.append(tweet[2])\n",
    "    tweetsTestLabel.append(tweet[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7b34e397",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(min_df = 5,\n",
    "                             max_df = 0.80,\n",
    "                             sublinear_tf=True,\n",
    "                             use_idf=True)\n",
    "\n",
    "train_vectors = vectorizer.fit_transform(tweetsTrainData)\n",
    "test_vectors = vectorizer.transform(tweetsTestData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "be2abb49",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifierNB = MultinomialNB()\n",
    "classifierNB.fit(train_vectors, tweetsTrainLabel)\n",
    "predictionNB=classifierNB.predict(test_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d2e93ae4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for MultinomialNB()\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.56      0.07      0.13       198\n",
      "     neutral       0.47      0.37      0.41       451\n",
      "    positive       0.57      0.84      0.68       577\n",
      "\n",
      "    accuracy                           0.54      1226\n",
      "   macro avg       0.53      0.43      0.41      1226\n",
      "weighted avg       0.53      0.54      0.49      1226\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Results for MultinomialNB()\")\n",
    "print(classification_report(tweetsTestLabel, predictionNB))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dcc805cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import sys\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report\n",
    "import csv\n",
    "\n",
    "tweetsTest = pd.read_csv(\"test.csv\", sep=',', header = 1)\n",
    "tweetsTrain = pd.read_csv(\"train.csv\", sep=',', header = 1)\n",
    "\n",
    "classes = ['negative', 'positive', 'neutral']\n",
    "stop_words = ['ourselves', 'hers', 'between', 'yourself', 'but', 'again', 'there', 'about', 'once', 'during', 'out', 'very', 'having', 'with', 'they', 'own', 'an', 'be', 'some', 'for', 'do', 'its', 'yours', 'such', 'into', 'of', 'most', 'itself', 'other', 'off', 'is', 's', 'am', 'or', 'who', 'as', 'from', 'him', 'each', 'the', 'themselves', 'until', 'below', 'are', 'we', 'these', 'your', 'his', 'through', 'don', 'nor', 'me', 'were', 'her', 'more', 'himself', 'this', 'down', 'should', 'our', 'their', 'while', 'above', 'both', 'up', 'to', 'ours', 'had', 'she', 'all', 'no', 'when', 'at', 'any', 'before', 'them', 'same', 'and', 'been', 'have', 'in', 'will', 'on', 'does', 'yourselves', 'then', 'that', 'because', 'what', 'over', 'why', 'so', 'can', 'did', 'not', 'now', 'under', 'he', 'you', 'herself', 'has', 'just', 'where', 'too', 'only', 'myself', 'which', 'those', 'i', 'after', 'few', 'whom', 't', 'being', 'if', 'theirs', 'my', 'against', 'a', 'by', 'doing', 'it', 'how', 'further', 'was', 'here', 'than']\n",
    "\n",
    "tweetsTrainData = []\n",
    "tweetsTrainLabel = []\n",
    "\n",
    "tweetsTestId = []\n",
    "tweetsTestData = []\n",
    "tweetsTestLabel = []\n",
    "\n",
    "for index, tweet in tweetsTrain.iterrows():\n",
    "    tweet[2] = re.sub(r'@\\w+', '', tweet[2])\n",
    "    querywords = tweet[2].split()\n",
    "    resultwords  = [word for word in querywords if word.lower() not in stop_words]\n",
    "    tweet[2] = ' '.join(resultwords)\n",
    "    tweetsTrainData.append(tweet[2])\n",
    "    tweetsTrainLabel.append(tweet[1])\n",
    "\n",
    "for index, tweet in tweetsTest.iterrows():\n",
    "    if str(tweet[1]) != \"nan\":\n",
    "        tweet[1] = re.sub(r'@\\w+', '', tweet[1])\n",
    "        querywords = tweet[1].split()\n",
    "        resultwords  = [word for word in querywords if word.lower() not in stop_words]\n",
    "        tweet[1] = ' '.join(resultwords)\n",
    "        tweetsTestData.append(tweet[1])\n",
    "        tweetsTestId.append(tweet[0])\n",
    "    \n",
    "vectorizer = TfidfVectorizer(min_df = 5,\n",
    "                             max_df = 0.80,\n",
    "                             sublinear_tf=True,\n",
    "                             use_idf=True)\n",
    "\n",
    "train_vectors = vectorizer.fit_transform(tweetsTrainData)\n",
    "test_vectors = vectorizer.transform(tweetsTestData)\n",
    "\n",
    "classifierNB = MultinomialNB()\n",
    "classifierNB.fit(train_vectors, tweetsTrainLabel)\n",
    "predictionNB=classifierNB.predict(test_vectors)\n",
    "\n",
    "header = {'Id': tweetsTestId, 'Category': predictionNB}\n",
    "result = pd.DataFrame(data = header)\n",
    "result.to_csv('result.csv', sep = ',', columns=['Id', 'Category'], index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6634f47",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
